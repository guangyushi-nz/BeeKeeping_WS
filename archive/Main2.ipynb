{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4608835-74b7-477e-9d46-3d15f54aaddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import scipy.fftpack as fft\n",
    "from scipy.signal import get_window\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca85e9e-f341-4912-92af-7f5a10039226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 48000Hz\n",
      "Audio duration: 155.352s\n"
     ]
    }
   ],
   "source": [
    "audio_file = \"beeAudioFile.mp3\"\n",
    "audio_data, sampling_rate = librosa.load(audio_file, sr=None)  # sr=None to get the original sampling rate\n",
    "\n",
    "# Play the audio using IPython's Audio\n",
    "# ipd.Audio(audio_data, rate=sampling_rate)\n",
    "\n",
    "print(\"Sample rate: {0}Hz\".format(sampling_rate))\n",
    "print(\"Audio duration: {0}s\".format(len(audio_data) / sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b2a17af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen_sampling_rate: 44100Hz\n",
      "no_queen_sampling_rate: 44100Hz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "queen_audio_file = \"QueenBee_Testing_15mins.wav\"\n",
    "queen_audio_data, queen_sampling_rate = librosa.load(queen_audio_file, sr=None)  # sr=None to get the original sampling rate\n",
    "\n",
    "# resample for no queen part\n",
    "no_queen_audio_file = \"No_QueenBee_Testing_15mins.wav\"\n",
    "no_queen_audio_data, no_queen_sampling_rate = librosa.load(no_queen_audio_file, sr=None)  # sr=None to get the original sampling rate\n",
    "\n",
    "#sample_rate, audio = librosa.load(\"beeAudioFile.mp3\")\n",
    "print(\"queen_sampling_rate: {0}Hz\".format(queen_sampling_rate))\n",
    "print(\"no_queen_sampling_rate: {0}Hz\".format(no_queen_sampling_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84d39e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440000\n",
      "480000\n"
     ]
    }
   ],
   "source": [
    "# Define segment length and hop length in seconds\n",
    "segment_length = 30  # seconds\n",
    "hop_length  = 10     # seconds (hop_length of 10 seconds)\n",
    "\n",
    "# Calculate frame length and hop length in samples\n",
    "frame_length = int(segment_length * queen_sampling_rate)\n",
    "hop_length_samples = int(hop_length * queen_sampling_rate)\n",
    "\n",
    "print(frame_length)\n",
    "print(hop_length_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63c3a023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queen Segments saved to Array.\n",
      "No Queen Segments saved to Array.\n"
     ]
    }
   ],
   "source": [
    "# generate queen segment\n",
    "\n",
    "# Iterate over the audio and create overlapping segments\n",
    "queen_segments = []\n",
    "start_sample = 0\n",
    "\n",
    "while start_sample + frame_length <= len(queen_audio_data):\n",
    "    segment = queen_audio_data[start_sample:start_sample + frame_length]\n",
    "    queen_segments.append(segment)\n",
    "    start_sample += hop_length_samples\n",
    "\n",
    "print(\"Queen Segments saved to Array.\")\n",
    "\n",
    "# 2000,  MFCC for each, 2000 MFCC features,  divded features, 1800 for training  200 for testing feed  LSTM and SVM\n",
    "\n",
    "# generate no queen segment\n",
    "\n",
    "# Iterate over the audio and create overlapping segments\n",
    "no_queen_segments = []\n",
    "start_sample = 0\n",
    "\n",
    "while start_sample + frame_length <= len(no_queen_audio_data):\n",
    "    no_queen_segment = no_queen_audio_data[start_sample:start_sample + frame_length]\n",
    "    no_queen_segments.append(no_queen_segment)\n",
    "    start_sample += hop_length_samples\n",
    "\n",
    "print(\"No Queen Segments saved to Array.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88fdc370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen segments: 34\n",
      "no queen segments: 403\n"
     ]
    }
   ],
   "source": [
    "print(\"queen segments:\", len(queen_segments))\n",
    "print(\"no queen segments:\", len(no_queen_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb94a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Lib\\site-packages\\audiomentations\\core\\audio_loading_utils.py:37: UserWarning: c:\\Users\\hippo\\Desktop\\Master-BeeKeeping\\BeeKeeping_WS\\nature_noise.mp3 had to be resampled from 44100 hz to 48000 hz. This hurt execution time.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# augmentation\n",
    "\n",
    "import numpy as np\n",
    "from audiomentations import Compose, TimeStretch, PitchShift, AddBackgroundNoise\n",
    "\n",
    "# Define the augmentation transformations you want to apply\n",
    "augmentations = Compose([\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2, p=0.5),  # Adjust time duration\n",
    "    PitchShift(min_semitones=-2, max_semitones=2, p=0.5),  # Adjust pitch\n",
    "    AddBackgroundNoise(sounds_path=\"nature_noise.mp3\", p=0.5),  # Add background noise\n",
    "])\n",
    "\n",
    "# Augment queen segments\n",
    "augmented_queen_segments = []\n",
    "for segment in queen_segments:\n",
    "    augmented_segment = augmentations(samples=segment, sample_rate=queen_sampling_rate)\n",
    "    augmented_queen_segments.append(augmented_segment)\n",
    "\n",
    "# Augment no queen segments\n",
    "augmented_no_queen_segments = []\n",
    "for segment in no_queen_segments:\n",
    "    augmented_segment = augmentations(samples=segment, sample_rate=no_queen_sampling_rate)\n",
    "    augmented_no_queen_segments.append(augmented_segment)\n",
    "\n",
    "# Now you have augmented segments in augmented_queen_segments and augmented_no_queen_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb6dcaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen segments: 34\n",
      "no queen segments: 403\n"
     ]
    }
   ],
   "source": [
    "print(\"queen segments:\", len(augmented_queen_segments))\n",
    "print(\"no queen segments:\", len(augmented_no_queen_segments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generage mel spectrogram into Folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def save_spectrogram(audio_segments, folder_name):\n",
    "    \"\"\"\n",
    "    Generates and saves Mel spectrograms for a list of audio segments.\n",
    "    Skips saving if the image already exists.\n",
    "    :param audio_segments: List of audio segments (numpy arrays)\n",
    "    :param folder_name: Folder to save the spectrogram images\n",
    "    \"\"\"\n",
    "    # Ensure the folder exists\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    # Loop through each segment and generate a spectrogram\n",
    "    for i, segment in enumerate(audio_segments):\n",
    "        file_path = f'{folder_name}/spectrogram_{i}.png'\n",
    "        \n",
    "        # Check if the image already exists\n",
    "        if not os.path.exists(file_path):\n",
    "            # Generate Mel spectrogram\n",
    "            S = librosa.feature.melspectrogram(y=segment, sr=22050, n_mels=128, fmax=8000)\n",
    "            S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            librosa.display.specshow(S_dB, sr=22050, x_axis='time', y_axis='mel')\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            plt.title(f'Mel-frequency spectrogram {i}')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the figure\n",
    "            plt.savefig(file_path)\n",
    "            plt.close()\n",
    "\n",
    "# Example usage\n",
    "# Replace these with your actual lists of audio segments\n",
    "# save_spectrogram(augmented_queen_segments, 'queen_img')\n",
    "# save_spectrogram(augmented_no_queen_segments, 'queenless_img')\n",
    "\n",
    "# Example usage\n",
    "# Assuming `augmented_queen_segments` and `augmented_no_queen_segments` are your audio segment lists\n",
    "save_spectrogram(augmented_queen_segments, 'queen_img')\n",
    "save_spectrogram(augmented_no_queen_segments, 'queenless_img')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df77e4f",
   "metadata": {},
   "source": [
    "extract MFCC features and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35ba3c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen segments: 34\n",
      "no queen segments: 403\n"
     ]
    }
   ],
   "source": [
    "def extract_mfcc_and_label(audio_segment, label):\n",
    "    # Extract MFCC features from the audio segment\n",
    "    mfccs = librosa.feature.mfcc(y=audio_segment, sr=queen_sampling_rate, n_mfcc=13, hop_length=hop_length_samples)\n",
    "    \n",
    "    return mfccs, label\n",
    "\n",
    "queen_mfccs = []  # To store MFCC features\n",
    "queen_labels = []  # To store labels (1 for \"queen\")\n",
    "\n",
    "for segment in augmented_queen_segments:\n",
    "    mfcc, label = extract_mfcc_and_label(segment, 1)  # Label as 1 for \"queen\"\n",
    "    queen_mfccs.append(mfcc)\n",
    "    queen_labels.append(label)\n",
    "\n",
    "\n",
    "no_queen_mfccs = []  # To store MFCC features\n",
    "no_queen_labels = []  # To store labels (1 for \"queen\")\n",
    "\n",
    "for no_queen_segment in augmented_no_queen_segments:\n",
    "    mfcc, label = extract_mfcc_and_label(no_queen_segment, 0)  # Label as 1 for \"queen\"\n",
    "    no_queen_mfccs.append(mfcc)\n",
    "    no_queen_labels.append(label)\n",
    "\n",
    "print(\"queen segments:\", len(queen_mfccs))\n",
    "print(\"no queen segments:\", len(no_queen_mfccs))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d32bd291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen_mfccs:\n",
      "[[-9.0834021e+02 -4.0651260e+02 -3.7499939e+02 -5.4887689e+02]\n",
      " [ 0.0000000e+00  1.7578656e+02  1.5964374e+02  1.2885406e+02]\n",
      " [ 0.0000000e+00 -2.5732170e+01 -4.4139931e+01  1.8598820e+01]\n",
      " [ 0.0000000e+00  6.0004074e+01  7.1312477e+01  4.3563576e+01]\n",
      " [ 0.0000000e+00 -3.9786682e+01 -4.7004913e+01  5.7934742e+00]\n",
      " [ 0.0000000e+00  3.0369919e+01  2.5160576e+01 -6.6570635e+00]\n",
      " [ 0.0000000e+00 -1.9473936e+01 -1.4755440e+01  8.8173813e-01]\n",
      " [ 0.0000000e+00  3.2558167e+01  3.3551048e+01  1.5242510e-01]\n",
      " [ 0.0000000e+00 -3.9381802e+00 -2.1995125e+01  3.5655100e+00]\n",
      " [ 0.0000000e+00  4.4144449e+00  7.1385679e+00  1.2887930e+01]\n",
      " [ 0.0000000e+00 -1.7217085e-01  1.7178435e+00  4.8987284e+00]\n",
      " [ 0.0000000e+00  7.0732756e+00 -2.9608059e+00  6.6356955e+00]\n",
      " [ 0.0000000e+00 -9.4648170e-01 -2.2162137e+00  1.3756098e+01]]\n",
      "no_queen_mfccs:\n",
      "[[-7.0478857e+02 -2.8825006e+02 -4.1513882e+02 -2.5544597e+02]\n",
      " [ 0.0000000e+00  1.7309164e+02  1.5318442e+02  1.4112778e+02]\n",
      " [ 0.0000000e+00  1.8207117e+01  2.1010846e-01  2.7083923e+01]\n",
      " [ 0.0000000e+00  6.5556808e+01  2.3610090e+01  2.9641207e+01]\n",
      " [ 0.0000000e+00 -2.5953318e+01 -1.2958461e+01 -9.2976494e+00]\n",
      " [ 0.0000000e+00  3.4650990e+01  1.5875336e+01  2.6043794e+00]\n",
      " [ 0.0000000e+00 -1.2957095e+01 -5.4136114e+00 -3.6729374e+00]\n",
      " [ 0.0000000e+00  1.1055464e+01 -2.9944463e+00  3.6220870e+00]\n",
      " [ 0.0000000e+00 -1.0297253e+01 -8.7165146e+00 -1.9775188e+00]\n",
      " [ 0.0000000e+00  6.6638821e-01 -5.5273571e+00 -2.2731278e+00]\n",
      " [ 0.0000000e+00 -6.1085997e+00 -1.0448034e+01 -4.3828192e-01]\n",
      " [ 0.0000000e+00 -4.0045576e+00 -7.1801062e+00  6.9990087e+00]\n",
      " [ 0.0000000e+00 -8.3567362e+00 -3.9481685e+00  3.9693127e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"queen_mfccs:\")\n",
    "print(queen_mfccs[0])\n",
    "print(\"no_queen_mfccs:\")\n",
    "print(no_queen_mfccs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb09588",
   "metadata": {},
   "source": [
    "queen_mfccs:\n",
    "[[-9.0834021e+02 -4.0651260e+02 -3.7499939e+02 -5.4887689e+02]\n",
    " [ 0.0000000e+00  1.7578656e+02  1.5964374e+02  1.2885406e+02]\n",
    " [ 0.0000000e+00 -2.5732170e+01 -4.4139931e+01  1.8598820e+01]\n",
    " [ 0.0000000e+00  6.0004074e+01  7.1312477e+01  4.3563576e+01]\n",
    " [ 0.0000000e+00 -3.9786682e+01 -4.7004913e+01  5.7934742e+00]\n",
    " [ 0.0000000e+00  3.0369919e+01  2.5160576e+01 -6.6570635e+00]\n",
    " [ 0.0000000e+00 -1.9473936e+01 -1.4755440e+01  8.8173813e-01]\n",
    " [ 0.0000000e+00  3.2558167e+01  3.3551048e+01  1.5242510e-01]\n",
    " [ 0.0000000e+00 -3.9381802e+00 -2.1995125e+01  3.5655100e+00]\n",
    " [ 0.0000000e+00  4.4144449e+00  7.1385679e+00  1.2887930e+01]\n",
    " [ 0.0000000e+00 -1.7217085e-01  1.7178435e+00  4.8987284e+00]\n",
    " [ 0.0000000e+00  7.0732756e+00 -2.9608059e+00  6.6356955e+00]\n",
    " [ 0.0000000e+00 -9.4648170e-01 -2.2162137e+00  1.3756098e+01]]\n",
    "no_queen_mfccs:\n",
    "[[-620.7817     -246.7279     -376.80215    -177.33191   ]\n",
    " [   0.          172.13995     145.6271      130.23778   ]\n",
    " [   0.           19.31228       4.8429527    20.572308  ]\n",
    " [   0.           58.24466      19.944237     20.364254  ]\n",
    " [   0.          -26.18211     -11.411879      2.146392  ]\n",
    " [   0.           28.36429       6.484435      2.1418092 ]\n",
    " [   0.          -13.176889     -0.63723433   -2.3569534 ]\n",
    " [   0.            3.0853925    -4.836183     -1.0625077 ]\n",
    " [   0.           -8.8347435    -5.7978144    -1.1277297 ]\n",
    " [   0.           -1.2606636    -3.86494      -4.05499   ]\n",
    " [   0.            1.0070633    -8.360489     -4.835867  ]\n",
    " [   0.           -6.2654414    -8.794887     -3.7851849 ]\n",
    " [   0.           -4.145375     -5.0080633    -5.352196  ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e90cfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (437, 13, 4)\n",
      "Shape of y: (437,)\n"
     ]
    }
   ],
   "source": [
    "# Combine the data from \"queen\" and \"no queen\" segments and labels:\n",
    "\n",
    "X = np.vstack((queen_mfccs, no_queen_mfccs))\n",
    "y = np.hstack((queen_labels, no_queen_labels))\n",
    "\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# The audio is divided into segments with a hop length of 10 seconds, \n",
    "# which results in 3 time steps per frame. \n",
    "# When you add the 0th coefficient, you have a total of 4 coefficients per frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71db9479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (437, 52)\n"
     ]
    }
   ],
   "source": [
    "# Reshape X to have two dimensions\n",
    "X = X.reshape(X.shape[0], -1)  # Flatten the last two dimensions\n",
    "\n",
    "# Now, X will have a shape of (437, 13 * 4)\n",
    "print(\"Shape of X:\", X.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42b44531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MFCC Features: 98.86%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        78\n",
      "           1       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.99        88\n",
      "   macro avg       0.95      0.99      0.97        88\n",
      "weighted avg       0.99      0.99      0.99        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier (you can choose different kernels and parameters)\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print the classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of MFCC Features: {accuracy * 100:.2f}%\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68517ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LPC order (you can adjust this)\n",
    "lpc_order = 12\n",
    "\n",
    "# Extract LPC features for augmented queen segments\n",
    "lpc_queen_segments = []\n",
    "for segment in augmented_queen_segments:\n",
    "    # Apply LPC analysis using librosa.lpc\n",
    "    lpc_coefficients = librosa.lpc(segment, order=lpc_order)\n",
    "    lpc_queen_segments.append(lpc_coefficients)\n",
    "\n",
    "# Extract LPC features for augmented no queen segments\n",
    "lpc_no_queen_segments = []\n",
    "for segment in augmented_no_queen_segments:\n",
    "    # Apply LPC analysis using librosa.lpc\n",
    "    lpc_coefficients = librosa.lpc(segment, order=lpc_order)\n",
    "    lpc_no_queen_segments.append(lpc_coefficients)\n",
    "\n",
    "# Now you have LPC features for both queen and no queen segments.\n",
    "# lpc_queen_segments and lpc_no_queen_segments are lists of LPC coefficients for each segment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "516ecbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming '1' represents \"queen\" and '0' represents \"no queen\"\n",
    "queen_labels = [1] * len(lpc_queen_segments)\n",
    "no_queen_labels = [0] * len(lpc_no_queen_segments)\n",
    "\n",
    "# Combine queen and no queen data and labels\n",
    "X = np.vstack((lpc_queen_segments, lpc_no_queen_segments))\n",
    "y = np.hstack((queen_labels, no_queen_labels))\n",
    "\n",
    "# Find rows with NaN or infinity values in X\n",
    "nan_inf_indices = np.isnan(X).any(axis=1) | np.isinf(X).any(axis=1)\n",
    "\n",
    "# Remove rows with NaN or infinity values from X and y\n",
    "X = X[~nan_inf_indices]\n",
    "y = y[~nan_inf_indices]\n",
    "\n",
    "# Now, X_clean and y_clean contain the data with NaN and infinity rows removed\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "974ecdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LPC Features: 88.51%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94        77\n",
      "           1       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.89        87\n",
      "   macro avg       0.44      0.50      0.47        87\n",
      "weighted avg       0.78      0.89      0.83        87\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Create an SVM classifier (you can choose different kernels and parameters)\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and print the classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of LPC Features: {accuracy * 100:.2f}%\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
